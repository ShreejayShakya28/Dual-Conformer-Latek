@misc{conformer,
      title={Conformer: Convolution-augmented Transformer for Speech Recognition}, 
      author={Anmol Gulati and James Qin and Chung-Cheng Chiu and Niki Parmar and Yu Zhang and Jiahui Yu and Wei Han and Shibo Wang and Zhengdong Zhang and Yonghui Wu and Ruoming Pang},
      year={2020},
      eprint={2005.08100},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/2005.08100}, 
}

@misc{speechLLM,
      title={A Survey on Speech Large Language Models}, 
      author={Jing Peng and Yucheng Wang and Yangui Fang and Yu Xi and Xu Li and Xizhuo Zhang and Kai Yu},
      year={2025},
      eprint={2410.18908},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/2410.18908}, 
}

@inproceedings{espnet,
  title={ESPnet: End-to-End Speech Processing Toolkit},
  author={Watanabe, Shinji and Hori, Takaaki and Karita, Shigeki and Hayashi, Tomoki and others},
  booktitle={Proc. Interspeech},
  year={2018}
}

@misc{attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

}@misc{MHSA,
      title={Improving Transformer-based Conversational ASR by Inter-Sentential Attention Mechanism}, 
      author={Kun Wei and Pengcheng Guo and Ning Jiang},
      year={2022},
      eprint={2207.00883},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2207.00883}, 
}

@misc{mha,
      title={Multi-Head Attention: Collaborate Instead of Concatenate}, 
      author={Jean-Baptiste Cordonnier and Andreas Loukas and Martin Jaggi},
      year={2021},
      eprint={2006.16362},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2006.16362}, 
}

@article{styletalker,
  title={Style-Talker: Finetuning Audio Language Model and Style-Based Text-to-Speech Model for Fast Spoken Dialogue Generation},
  author={Li, Yinghao Aaron et al.},
  journal={arXiv preprint arXiv:2408.11849},
  year={2024}
}

@INPROCEEDINGS{speech-transformer,

  author={Dong, Linhao and Xu, Shuang and Xu, Bo},

  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 

  title={Speech-Transformer: A No-Recurrence Sequence-to-Sequence Model for Speech Recognition}, 

  year={2018},

  volume={},

  number={},

  pages={5884-5888},

  keywords={Hidden Markov models;Encoding;Training;Decoding;Speech recognition;Time-frequency analysis;Spectrogram;Speech Recognition;Sequence-to-Sequence;Attention;Transformer},

  doi={10.1109/ICASSP.2018.8462506}}

@inproceedings{TransformerVsRNN,
   title={A Comparative Study on Transformer vs RNN in Speech Applications},
   url={http://dx.doi.org/10.1109/ASRU46091.2019.9003750},
   DOI={10.1109/asru46091.2019.9003750},
   booktitle={2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)},
   publisher={IEEE},
   author={Karita, Shigeki and Chen, Nanxin and Hayashi, Tomoki and Hori, Takaaki and Inaguma, Hirofumi and Jiang, Ziyan and Someki, Masao and Soplin, Nelson Enrique Yalta and Yamamoto, Ryuichi and Wang, Xiaofei and Watanabe, Shinji and Yoshimura, Takenori and Zhang, Wangyou},
   year={2019},
   month=dec, pages={449–456} }

@article{tera,
   title={TERA: Self-Supervised Learning of Transformer Encoder Representation for Speech},
   volume={29},
   ISSN={2329-9304},
   url={http://dx.doi.org/10.1109/TASLP.2021.3095662},
   DOI={10.1109/taslp.2021.3095662},
   journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Liu, Andy T. and Li, Shang-Wen and Lee, Hung-yi},
   year={2021},
   pages={2351–2366} }

@article{wav2vec,
  title={wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations},
  author={Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
  journal={NeurIPS},
  year={2020}
}

@article{audiogpt2023,
  title={AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head},
  author={Huang, Xuankai and others},
  journal={arXiv preprint arXiv:2304.12995},
  year={2023}
}

@article{gpt,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  journal={OpenAI Blog},
  year={2018}
}

@inproceedings{bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={NAACL-HLT},
  year={2019}
}

% Table
@misc{cif,
      title={CIF: Continuous Integrate-and-Fire for End-to-End Speech Recognition}, 
      author={Linhao Dong and Bo Xu},
      year={2020},
      eprint={1905.11235},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1905.11235}, 
}

@misc{streaming-trans,
      title={Streaming Transformer ASR with Blockwise Synchronous Beam Search}, 
      author={Emiru Tsunoo and Yosuke Kashiwagi and Shinji Watanabe},
      year={2020},
      eprint={2006.14941},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/2006.14941}, 
}

@misc{streaming-trans-nobeam,
      title={Streaming automatic speech recognition with the transformer model}, 
      author={Niko Moritz and Takaaki Hori and Jonathan Le Roux},
      year={2020},
      eprint={2001.02674},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2001.02674}, 
}

@misc{macaron,
      title={Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View}, 
      author={Yiping Lu and Zhuohan Li and Di He and Zhiqing Sun and Bin Dong and Tao Qin and Liwei Wang and Tie-Yan Liu},
      year={2019},
      eprint={1906.02762},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1906.02762}, 
}
@misc{scaling-kaplan,
      title={Scaling Laws for Neural Language Models}, 
      author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
      year={2020},
      eprint={2001.08361},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2001.08361}, 
}

@misc{llama,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.13971}, 
}

@article{ctc-graves,
author = {Graves, Alex and Fernández, Santiago and Gomez, Faustino and Schmidhuber, Jürgen},
year = {2006},
month = {01},
pages = {369-376},
title = {Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks},
volume = {2006},
journal = {ICML 2006 - Proceedings of the 23rd International Conference on Machine Learning},
doi = {10.1145/1143844.1143891}
}

@article{bahl,
author = {Bahl, Lalit and Jelinek, Frederick and Mercer, Robert},
year = {1983},
month = {04},
pages = {179 - 190},
title = {A Maximum Likelihood Approach to Continuous Speech Recognition},
volume = {PAMI-5},
isbn = {9781558601246},
journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
doi = {10.1109/TPAMI.1983.4767370}
}

@book{sangam-book,
  author       = {Sebastian Raschka},
  title        = {Build A Large Language Model (From Scratch)},
  publisher    = {Manning},
  year         = {2024},
  isbn         = {978-1633437166},
  url          = {https://www.manning.com/books/build-a-large-language-model-from-scratch},
  github       = {https://github.com/rasbt/LLMs-from-scratch}
}

@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}
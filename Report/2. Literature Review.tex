\newpage


\section{\MakeUppercase{Literature Review}}
Spoken language processing has become a fundamental midpoint in that it has become the place where two potentially evolutionary technologies converge: high-accuracy Automatic Speech Recognition (ASR) systems and reasoning-powered Large Language Models (LLMs). This is the recent stage of a long history of development whereby primitive systems of speech transcription have evolved into systems able to not only understand but also participate in a conversational environment. On one hand, an increasing body of current research supports the idea of closely integrated, end-to-end Speech-LLMs \cite{speechLLM}, including, but not limited to the AudioGPT \cite{audiogpt2023} and Style-Talker models \cite{styletalker}. These systems aim at deep multi-modal fusion, creating LLMs capable of being fed and processed with raw or latent acoustic representations. Its goal is to realize a unified cognitive agent existing along the spectrum between acoustic waveform and intelligent response and is therefore capable of preserving paralinguistic information, such as prosody, tone and emotion, which are encoded in the speech signal but are lost when the speech is used in text transcription. Its supporters believe that this level of integration can be optimized in common against a single goal, with the wide range of linguistic and world knowledge of the LLM able to inform and, possibly, correct the process of creating acoustic perception online in the synergistic sense. However, this paradigm also comes with a high cost to system design (architectural rigidity that hampers independent component upgrades, a vast need of matched audio-text dialogue pairs, data that is relatively rare when compared to mono-modal datasets), and high-level of computational overhead that makes deployment and scaling difficult, as the entire monolithic model needs to be trained, fine-tuned, and simply served as an inseparable unit.

The modular, decoupled pipeline, however, reflects a classical concept of systems engineering: separation of concerns. The methodological basis of the current work is this approach that forms spoken language systems as the sequential modular assembly. A special, high-performance ASR engine is a transducer device used specifically, which converts the acoustic speech signal to a discrete textual transcript. This transcript is then an interoperable, standardized interface to a separate, perhaps generic, LLM module that performs comprehension, reasoning, and response generation. The strength of this architecture is its built-in modularity and independence, which allows the ASR and LLM components to be researched, developed, trained on domain-specific data, optimized to run on specific hardware, and deployed at scale without the need to depend on each other. This encourages such a profound specialization; the ASR can be optimized exhaustively towards acoustic robustness, noise invariance, and speaker adaptation using large volumes of transcribed speech, whereas the LLM can be carefully fine-tuned to domain-specific dialogue, safety, and style generation using the larger universe of text-based conversations and documents. As a result, the system architecture will achieve exceptional flexibility, maintainability, and scalability since the components can be changed, upgraded or even expanded across the horizontal depending on demand without requiring an entire retraining of the system. Despite the trade-offs inherent in this decoupled implementation, including a rise in latency of the pipeline, the possibility of error propagation where ASR transcription errors become unchangeable LLM input, and the sheer loss of non-textual acoustic information, this decoupled implementation strategy has many practical advantages. In voice-based assistants in limited-scope areas, audio and video stream content moderation, meeting transcription and summarization, and domain-specific query systems the advantages of robustness, simple programming, and scalability of operations tend to vastly outweigh the disadvantages.

The modern discussion of integration-related strategies is based on the groundbreaking neural architecture breakthroughs, whose primary force is the Transformer. Applying the initial Transformer architecture to ASR immediately faced a rather fundamental bottleneck: the computational complexity of the self-attention mechanism scales quadratically $O(n^2)$ with sequence length). Order of magnitude longer than text sequences are speech signals, which can be represented as a sequence of frames (such as 100 Hz); this makes the vanilla training and inference of Transformer prohibitively costly. This difficulty prompted more speech-specific variants to be developed in an efficient manner. The most effective of them is the Conformer (Convolution-Augmented Transformer) by Gulati et al. (2020) \cite{conformer}, which has become the default encoder in the state-of-the-art ASR. The cleverness of The Conformer is that the hybrid design is specifically touched upon in the dual nature of speech. A typical Conformer block is a series of feed forward, multi-headed self-attention improved from "Multi-Head Attention: Collaborate Instead of Concatenate" \cite{MHSA,mha}, convolution, and feed forward modules. The self-attention element represents long-range, global dependencies and contextual relationships throughout an utterance, which is critical to solving semantic ambiguity, and, in parallel, the convolution module finds it easy to model narrow, high time-density correlations in the acoustic signal, viz phonetic patterns, spectral transitions. A combination of these complementary capabilities allows the Conformer to recognize speech as a signal with high local stationarity, best represented by convolutional neural networks, embedded in a hierarchical linguistic structure, best represented by attention mechanisms. This new invention in architecture has given the strong, efficient and very accurate speech encoding mandatory to present systems.

Parallel with this, the Transformer architecture similarly brought a parallel revolution in natural language processing. Transformers trained in decoder only on a cause-and-effect language-modeling task (predicting the next token) were found to be able to scale in a way never seen before. Empirical validation of this came with GPT series ( Generative Pre-trained Transformer ), in which Radford et al. (2018, 2019) \cite{gpt} scaled up such models, in both parameters and data, and found them to have impressive emergent capabilities in a variety of reasoning, instruction-following and in-context learning, with no task-specific creation of architectures. The quantitative roadmap provided by the empirical study of scaling laws by Kaplan et al. (2020) (\cite{scaling-kaplan}) provided an essential quantitative understanding of predictable power-law enhancesments in performance with model size, dataset size, and computing budget. This scaling thesis immediately gave rise to the age of LLMs like GPT-3, PaLM, and LLaMA \cite{llama}, whose parametric knowledge and generative fluency are the basis of modern conversational AI. The presence of these two transformer-based powerful pillars, high-accuracy Conformer ASR and GPT-like LLM with its knowledge, naturally resulted in the current area of research interest in integrating them.

Before the development of transformer-based architectures, the first generation of deep learning used on automatic speech recognition (ASR) was not so much a game-changer as it was an improvement. Instead of a complete replacement of the statistical framework, deep learning presented a very successful component replacement into the already successful paradigm. This shift was formalised by the breakthrough study by Hinton et al. (2012) which showed that a hybrid deep neural network-hidden Markov model (DNN-HMM) architecture was effective. In this scheme a neural network, implementing the deep neural network, replaced the Gaussian mixture model (GMM) as the model of estimations of the posterior probabilities of HMM states (senones). Although the HMM still addressed temporal dynamics, and decoded sequence, the deep, hierarchical non-linear layers of the DNN network had had highly enhanced abilities to learn discriminative feature presentation directly off acoustic frames, and provided substantial gains in the word error rate. At the same time, the search of architectural simplification initiated an interest in the real end-to-end systems. Graves et al. (2006) proposed the Connectionist Temporal Classification (CTC) \cite{ctc-graves} algorithm which solved the fundamental temporal alignment problem between sequences of variable length of input and output sequences by introducing a blank word, and by applying a dynamic-programming path-collapsing algorithm. This allowed single neural networks usually recurrent networks like long short term memory (LSTM) networks to directly learn the acoustic features sequence-to-character/wordpiece sequence mapping, which simplifies the training pipeline and lets the field be more aligned to end-to-end learning principles.

The developments of these deep-learning replaced the statistical paradigm of long dominance in ASR, which is the GMM-HMM paradigm. Having been theorized by Bahl, Jelinek, and Mercer (1983)\cite{bahl}, this paradigm redefined the speech recognition as a Bayesian inference problem. Bayes theorem was used to divide the task into an acoustic model, which is, P(A 0 W) and a language model, P(W). A GMM-HMM was used to implement the acoustic model with the hidden Markov models modeling sequence of context-dependent sub-word units (triphones), and the probability density of acoustic feature vectors (e.g. MFCCs) at each state being modeled by Gaussian mixture models. Typically, an n-gram model, the language model, offered prior knowledge of likelihoods of word-sequences. This statistical model, despite being an impressive engineering accomplishment and allowing its commercial use overall, did not achieve performance improvements indefinitely because the representational capacity of GMMs was limited, and HMMs relied on the assumption of conditional-independence.

The very idea of statistical era was the direct reaction to the inadequacy of the original rule-based approach. The acoustic-phonetic paradigm dominated in the 1960s and 1970s. It was influenced by linguistic and perceptual theories to build recognisers, and devised explicit rules and detectors that would scale invariant acoustic landmarks, usually formant frequencies and trajectories, to discrete phonemic categories. This bottom-up approach was not possible in the context of continuous speech due to the ubiquitous, context-dependent phenomenon of coarticulation. The articulatory gestures of successive sounds are similar leading to the acoustic realisation of a phoneme to differ dramatically with phonetic neighbours of that phoneme. This variability nullified the hypothesis of acoustic invariance and showed that the mapping of sound to symbol is highly contextualized and non-linear and so placing the field on probabilistic rather than deterministic methodologies.

Overall, the historical trend of ASR is one of failure, success, and improvement: at first, the unsuccessful results in handcrafted, rule-based detection, then, the successful results in engineered work of statistical decomposition, then, the component-level success obtained with the use of deep neural networks, and, lastly, the efficiency and architectural simplification of specialised transformers. Nowadays, the field of work is concerned with the strategic combination of these capabilities with the reasoning capabilities of the large language models. This undertaking falls in a decoupled, modular integration paradigm. We conjecture that in a wide range of practical applications that demand robustness, maintainability and scaleable implementation the principled separation between acoustic transduction and linguistic reasoning is a practical and better solution. We will present the actual application of a more advanced, production-scalable ASR module and a customized large language model, and thus embody the tangible performance and system-level benefits of this modular architectural ideology.


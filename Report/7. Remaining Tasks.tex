\newpage
\section{\MakeUppercase{Remaining Tasks}}
\subsection{Pipeline}
\begin{itemize}
	\item Implement FastAPI (or a lightweight custom API) to serve the ASR-to-LLM inference pipeline, returning transcribed text and generated responses.
	\item Create a web UI designed to allow easy user audio input and display the ASR output and LLM response.
\end{itemize}

\subsection{ASR Module}
\begin{itemize}
	\item The current system employs greedy decoding for simplicity and speed; however, we are exploring alternative decoding strategies such as beam search with language model fusion to improve transcription accuracy and reduce Word Error Rate (WER).
	\item The remaining task is to perform small-scale fine-tuning to yield slightly better transcription results and reduce Word Error Rate (WER).
\end{itemize}

\subsection{Small Language Model}
\textbf{Model Scaling, Parameter Reduction and Ablation studies}

The model needs to be downscaled from its base model with a parameter count of 124M to create more lightweight variants by removing each layer to create a smaller model. To understand the contribution of specific architectural components the removing or altering of individual parts of the model and observing the loss and generation quality of the model These includes Attention head pruning to remove attention heads to determine the minimum number of heads which gives satisfactory result.Feed forward Network reduction and Layer wise removal of model.

\textbf{Comparative  Training}

Every down-scaled model needs to undergo training to compare against the baseline by training the model architecture in the original dataset, monitoring training dynamics and see how evaluation metrics change with different configurations. The goal is to find the relationship between parameter count and model performance to visualize. 

\textbf{Impact Analysis and Metric Evaluation}

The analysis of trade-off between model size and performance capability needs to be done. Find the relationship between parameter count and model performance to visualize the impact. Sensitivity analysis to quantify which components have the highest impact on model accuracy and performance shows which components  are negotiable or not and the importance of individual components. Select the smallest possible architecture that shows satisfactory results.

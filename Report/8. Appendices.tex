\newpage
\section{\MakeUppercase{Appendices}} 
\label{sec:appendices}
\subsection*{Appendix A : Gantt Chart}
\addcontentsline{toc}{subsection}{Appendix A : Gantt Chart}
\begin{figure}[H]
    \centering
    \rotatebox{90}{ % Rotates the image by 90 degrees
        \includegraphics[scale=0.6]{Images/Others/gantt_chart_weekly_ticks.png}
    }
    \caption{Gantt Chart}
    \label{fig:Gantt Chart}
\end{figure}
\pagebreak

\subsection*{Appendix B : Project Budget}
\addcontentsline{toc}{subsection}{Appendix B : Project Budget}
\begin{table}[H]
    \caption{Project Budget}
    \centering
    \renewcommand{\arraystretch}{2.0}
    \setlength{\tabcolsep}{15pt}
    \begin{tabular}{|l|l|l|}
        \hline
        \textbf{SN.} & \textbf{Particular} & \textbf{Estimated Cost (Rs)}\\
        \hline
        1 & Printing and Documentation & 5000 \\
        \hline
        2 & Cloud Computing & 15000 \\
        \hline
        3 & Miscellaneous & 2000 \\
        \hline
         & \textbf{Total} & 22000\\
        \hline
    \end{tabular}
    \label{Project Budget}
\end{table}
\pagebreak

\subsection*{Appendix C : Multi Head Attention Mechanism}
\addcontentsline{toc}{subsection}{Appendix C : Multi Head Attention Mechanism}


\lstset{
	language=Python,
    basicstyle=\rmfamily\small, % Times New Roman
	breaklines=true,
	showstringspaces=false,
	frame=single,
	tabsize=4
}

\textbf{Mutli Head Attention Mechanism Implementation}

\begin{lstlisting}
import torch
import torch.nn as nn

class MultiHeadAttention(nn.Module):
	def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):
		super().__init__()
		assert d_out % num_heads == 0, "d_out must be divisible by n_heads"
	
		self.d_out = d_out
		self.num_heads = num_heads
		self.head_dim = d_out // num_heads  
		self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
		self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
		self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
		self.out_proj = nn.Linear(d_out, d_out)  
		self.dropout = nn.Dropout(dropout)
		self.register_buffer("mask", torch.triu(torch.ones(context_length, context_length), diagonal=1))
		
		def forward(self, x):
		b, num_tokens, d_in = x.shape
		
		keys = self.W_key(x) 
		queries = self.W_query(x)
		values = self.W_value(x)
		
		keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)
		values = values.view(b, num_tokens, self.num_heads, self.head_dim)
		queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)
		
		# Transpose: (b, num_tokens, num_heads, head_dim) to (b, num_heads, num_tokens, head_dim)
		keys = keys.transpose(1, 2)
		queries = queries.transpose(1, 2)
		values = values.transpose(1, 2)
		
		attn_scores = queries @ keys.transpose(2, 3) 
		
		mask_bool = self.mask.bool()[:num_tokens, :num_tokens]
		attn_scores.masked_fill_(mask_bool, -torch.inf)
		
		attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
		attn_weights = self.dropout(attn_weights)
		
		context_vec = (attn_weights @ values).transpose(1, 2)
		context_vec = context_vec.reshape(b, num_tokens, self.d_out)
		context_vec = self.out_proj(context_vec)  # optional projection
		
		return context_vec
\end{lstlisting}
\pagebreak

\subsection*{Appendix D : Model Architecture}
\addcontentsline{toc}{subsection}{Appendix D : Model Architecture}


\lstset{
	language=Python,
    basicstyle=\rmfamily\small, % Times New Roman
	breaklines=true,
	showstringspaces=false,
	frame=single,
	tabsize=2
}

\textbf{Model Architecture}

\begin{lstlisting}
class TransformerBlock(nn.Module):
	def __init__(self, cfg):
		super().__init__()
		self.att = MultiHeadAttention(
		d_in=cfg["emb_dim"],
		d_out=cfg["emb_dim"],
		context_length=cfg["context_length"],
		num_heads=cfg["n_heads"],
		dropout=cfg["drop_rate"],
		qkv_bias=cfg["qkv_bias"])
		self.ff = FeedForward(cfg)
		self.norm1 = LayerNorm(cfg["emb_dim"])
		self.norm2 = LayerNorm(cfg["emb_dim"])
		self.drop_resid = nn.Dropout(cfg["drop_rate"])

	def forward(self, x):
		# Shortcut connection for attention block
		shortcut = x
		x = self.norm1(x)
		x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]
		x = self.drop_resid(x)
		x = x + shortcut  # Add the original input back
		
		# Shortcut connection for feed-forward block
		shortcut = x
		x = self.norm2(x)
		x = self.ff(x)
		x = self.drop_resid(x)
		x = x + shortcut  # Add the original input back
		
		return x

class GPTModel(nn.Module):
	def __init__(self, cfg):
		super().__init__()
		self.tok_emb = nn.Embedding(cfg["vocab_size"], cfg["emb_dim"])
		self.pos_emb = nn.Embedding(cfg["context_length"], cfg["emb_dim"])
		self.drop_emb = nn.Dropout(cfg["drop_rate"])
		
		self.trf_blocks = nn.Sequential(
		*[TransformerBlock(cfg) for _ in range(cfg["n_layers"])])
		
		self.final_norm = LayerNorm(cfg["emb_dim"])
		self.out_head = nn.Linear(cfg["emb_dim"], cfg["vocab_size"], bias=False)

	def forward(self, in_idx):
		batch_size, seq_len = in_idx.shape
		tok_embeds = self.tok_emb(in_idx)
		pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))
		x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]
		x = self.drop_emb(x)
		x = self.trf_blocks(x)
		x = self.final_norm(x)
		logits = self.out_head(x)
		return logits
\end{lstlisting}
\pagebreak

\subsection*{Appendix E : Feed Forward ,Normalization and Activation}
\addcontentsline{toc}{subsection}{Appendix E : Feed Forward ,Normalization and Activation}


\lstset{
	language=Python,
    basicstyle=\rmfamily\small, % Times New Roman
	breaklines=true,
	showstringspaces=false,
	frame=single,
	tabsize=3
}

\textbf{Feed Forward ,Normalization and Activation}

\begin{lstlisting}
class LayerNorm(nn.Module):
	def __init__(self, emb_dim):
		super().__init__()
		self.eps = 1e-5
		self.scale = nn.Parameter(torch.ones(emb_dim))
		self.shift = nn.Parameter(torch.zeros(emb_dim))
	
	def forward(self, x):
		mean = x.mean(dim=-1, keepdim=True)
		var = x.var(dim=-1, keepdim=True, unbiased=False)
		norm_x = (x - mean) / torch.sqrt(var + self.eps)
		
		return self.scale * norm_x + self.shift

class GELU(nn.Module):
	def __init__(self):
		super().__init__()

	def forward(self, x):
		return 0.5 * x * (1 + torch.tanh(
		torch.sqrt(torch.tensor(2.0 / torch.pi)) *
		(x + 0.044715 * torch.pow(x, 3))
		))

	class FeedForward(nn.Module):
		def __init__(self, cfg):
		super().__init__()
		self.layers = nn.Sequential(
		nn.Linear(cfg["emb_dim"], 4 * cfg["emb_dim"]),
		GELU(),
		nn.Linear(4 * cfg["emb_dim"], cfg["emb_dim"]),
		)

	def forward(self, x):
		return self.layers(x)
\end{lstlisting}
\pagebreak


% \subsection*{Appendix C: Sobels Edge Detection Algorithm}
% \addcontentsline{toc}{subsection}{Appendix C: Sobels Algorithm}
% \pagebreak

% \subsection*{Appendix D: K-Means Clustering Algorithm for Pie Chart Segmentation}
% \addcontentsline{toc}{subsection}{Appendix D: K-Means Clustering Algorithm for Pie Chart Segmentation}
% \newpage
% \subsection*{Appendix E: Consultation Form}
% \addcontentsline{toc}{subsection}{Appendix E: Consultation Form}

% \newpage

% \subsection*{Appendix F: Summary of Plagiarism Report}
% \addcontentsline{toc}{subsection}{Appendix F: Summary of Plagiarism Report}
% \pagebreak
